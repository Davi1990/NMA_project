{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
    "  ]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "\n",
    "\n",
    "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
    "      or None to load all runs.\n",
    "    concat (bool) : If True, concatenate multiple runs in time\n",
    "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  # Get the list relative 0-based index of runs to use\n",
    "  if runs is None:\n",
    "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
    "  elif isinstance(runs, int):\n",
    "    runs = [runs]\n",
    "\n",
    "  # Get the first (1-based) run id for this experiment \n",
    "  offset = get_image_ids(name)[0]\n",
    "\n",
    "  # Load each run's data\n",
    "  bold_data = [\n",
    "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
    "  ]\n",
    "\n",
    "  # Optionally concatenate in time\n",
    "  if concat:\n",
    "    bold_data = np.concatenate(bold_data, axis=-1)\n",
    "\n",
    "  return bold_data\n",
    "\n",
    "\n",
    "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339, 360, 360)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# The download cells will store the data in nested directories starting here:\n",
    "HCP_DIR = \"./hcp\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "subjects = range(N_SUBJECTS)\n",
    "\n",
    "timeseries_rest = []\n",
    "for subject in subjects:\n",
    "  ts_concat = load_timeseries(subject, \"rest\")\n",
    "  timeseries_rest.append(ts_concat)\n",
    "\n",
    "timeseries_rest_array = np.array(timeseries_rest)\n",
    "\n",
    "all_fc_data = np.zeros((339, 64980))\n",
    "\n",
    "fc = np.zeros((N_SUBJECTS, N_PARCELS, N_PARCELS))\n",
    "for sub, ts in enumerate(timeseries_rest):\n",
    "  fc[sub] = np.corrcoef(ts)\n",
    "\n",
    "fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's predict the behavioural performance (e.g. fluid intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 237 samples, validate on 102 samples\n",
      "Epoch 1/3\n",
      "237/237 [==============================] - 199s 841ms/sample - loss: 279.9528 - mean_squared_error: 279.9528 - val_loss: 281.4314 - val_mean_squared_error: 281.4314\n",
      "Epoch 2/3\n",
      "237/237 [==============================] - 181s 763ms/sample - loss: 277.7547 - mean_squared_error: 277.7546 - val_loss: 281.4314 - val_mean_squared_error: 281.4314\n",
      "Epoch 3/3\n",
      "237/237 [==============================] - 190s 803ms/sample - loss: 277.7547 - mean_squared_error: 277.7546 - val_loss: 281.4314 - val_mean_squared_error: 281.4314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8610689e80>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "all_behav_data = pd.read_csv('filtered_behavioural.csv', sep=';')\n",
    "behav = 'PMAT24_A_CR'\n",
    "all_behav_data[behav].fillna((all_behav_data[behav].mean()), inplace=True)\n",
    "\n",
    "\n",
    "X = fc\n",
    "y = np.array(all_behav_data[behav])\n",
    "\n",
    "\n",
    "X = X.reshape(-1,360, 360, 1)\n",
    "                \n",
    "\n",
    "input_shape = (360, 360)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=3, validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to predict gender instead     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 237 samples, validate on 102 samples\n",
      "WARNING:tensorflow:From /home/davide/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "237/237 [==============================] - 224s 946ms/sample - loss: 0.6945 - acc: 0.4852 - val_loss: 0.7016 - val_acc: 0.4314\n",
      "Epoch 2/20\n",
      "237/237 [==============================] - 193s 813ms/sample - loss: 0.6938 - acc: 0.5148 - val_loss: 0.6902 - val_acc: 0.5637\n",
      "Epoch 3/20\n",
      "237/237 [==============================] - 186s 784ms/sample - loss: 0.6894 - acc: 0.5612 - val_loss: 0.6815 - val_acc: 0.5686\n",
      "Epoch 4/20\n",
      "237/237 [==============================] - 188s 792ms/sample - loss: 0.6854 - acc: 0.5422 - val_loss: 0.6819 - val_acc: 0.5588\n",
      "Epoch 5/20\n",
      "237/237 [==============================] - 187s 789ms/sample - loss: 0.6826 - acc: 0.5506 - val_loss: 0.6776 - val_acc: 0.5686\n",
      "Epoch 6/20\n",
      "237/237 [==============================] - 209s 880ms/sample - loss: 0.6826 - acc: 0.5886 - val_loss: 0.6773 - val_acc: 0.5686\n",
      "Epoch 7/20\n",
      "237/237 [==============================] - 179s 754ms/sample - loss: 0.6778 - acc: 0.5338 - val_loss: 0.6751 - val_acc: 0.5735\n",
      "Epoch 8/20\n",
      "237/237 [==============================] - 187s 787ms/sample - loss: 0.6817 - acc: 0.5886 - val_loss: 0.6753 - val_acc: 0.5882\n",
      "Epoch 9/20\n",
      "237/237 [==============================] - 194s 817ms/sample - loss: 0.6749 - acc: 0.6076 - val_loss: 0.6729 - val_acc: 0.5931\n",
      "Epoch 10/20\n",
      "237/237 [==============================] - 206s 868ms/sample - loss: 0.6680 - acc: 0.5992 - val_loss: 0.6794 - val_acc: 0.5686\n",
      "Epoch 11/20\n",
      "237/237 [==============================] - 179s 755ms/sample - loss: 0.6693 - acc: 0.6498 - val_loss: 0.6711 - val_acc: 0.5931\n",
      "Epoch 12/20\n",
      "237/237 [==============================] - 186s 786ms/sample - loss: 0.6674 - acc: 0.6287 - val_loss: 0.6902 - val_acc: 0.5392\n",
      "Epoch 13/20\n",
      "237/237 [==============================] - 204s 860ms/sample - loss: 0.6717 - acc: 0.6076 - val_loss: 0.6715 - val_acc: 0.5833\n",
      "Epoch 14/20\n",
      "237/237 [==============================] - 178s 750ms/sample - loss: 0.6637 - acc: 0.6329 - val_loss: 0.6657 - val_acc: 0.5882\n",
      "Epoch 15/20\n",
      "237/237 [==============================] - 188s 794ms/sample - loss: 0.6615 - acc: 0.6224 - val_loss: 0.6727 - val_acc: 0.5980\n",
      "Epoch 16/20\n",
      "237/237 [==============================] - 192s 810ms/sample - loss: 0.6647 - acc: 0.5717 - val_loss: 0.6661 - val_acc: 0.5980\n",
      "Epoch 17/20\n",
      "237/237 [==============================] - 218s 920ms/sample - loss: 0.6598 - acc: 0.6308 - val_loss: 0.6628 - val_acc: 0.5686\n",
      "Epoch 18/20\n",
      "237/237 [==============================] - 177s 748ms/sample - loss: 0.6580 - acc: 0.5949 - val_loss: 0.6701 - val_acc: 0.6029\n",
      "Epoch 19/20\n",
      "237/237 [==============================] - 191s 807ms/sample - loss: 0.6556 - acc: 0.6160 - val_loss: 0.6643 - val_acc: 0.5882\n",
      "Epoch 20/20\n",
      "237/237 [==============================] - 191s 805ms/sample - loss: 0.6505 - acc: 0.6055 - val_loss: 0.6603 - val_acc: 0.5931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1172fbaa90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "all_behav_data = pd.read_csv('filtered_behavioural.csv', sep=';')\n",
    "all_behav_data['Gender'].replace(['F','M'],[0,1],inplace=True)\n",
    "\n",
    "behav = 'Gender'\n",
    "all_behav_data[behav].fillna((all_behav_data[behav].mean()), inplace=True)\n",
    "X = fc\n",
    "y = np.array(all_behav_data[behav])\n",
    "\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "X = X.reshape(-1,360, 360, 1)\n",
    "                \n",
    "\n",
    "input_shape = (360, 360)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy',\n",
    " #             optimizer='adam',\n",
    "  #            metrics=['accuracy'])\n",
    "\n",
    "opt = SGD(lr=0.001)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=20, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
